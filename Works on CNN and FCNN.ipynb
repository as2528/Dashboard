{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677737c7-29c3-4b94-aba3-0ddcbd46aec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import tempfile\n",
    "import os\n",
    "import cpuinfo\n",
    "import zipfile\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "from moviepy.editor import VideoFileClip\n",
    "from matplotlib import animation\n",
    "from sklearn.decomposition import PCA\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import shutil\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from PIL import Image, ImageFile\n",
    "import logging \n",
    "\n",
    "import os\n",
    "def safe_output(path):\n",
    "    if os.path.isdir(path):\n",
    "        raise ValueError(f\"Expected a file, got a directory: {path}\")\n",
    "    return path\n",
    "\n",
    "\n",
    "logging.getLogger('PIL.TiffImagePlugin').setLevel(logging.INFO)\n",
    "\n",
    "OUTPUT_DIR = os.path.abspath(\"outputs\")\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "layer_map = {\n",
    "    \"Linear\": lambda in_dim, out_dim: nn.Linear(int(in_dim), int(out_dim)),\n",
    "    \"Conv2d\": lambda in_dim, out_dim: nn.Conv2d(int(in_dim), int(out_dim), kernel_size=3, padding=1),  \n",
    "    \"MaxPool2d\": lambda *_: nn.MaxPool2d(kernel_size=2),  \n",
    "    \"AvgPool2d\": lambda *_: nn.AvgPool2d(kernel_size=2),  \n",
    "    \"Dropout\": lambda p=0.5, *_: nn.Dropout(float(p)),\n",
    "    \"ReLU\": lambda *_: nn.ReLU(),\n",
    "    \"Tanh\": lambda *_: nn.Tanh(),\n",
    "    \"Sigmoid\": lambda *_: nn.Sigmoid(),\n",
    "    \"Flatten\": lambda *_: nn.Flatten(),\n",
    "    \"Softmax\": lambda *_: nn.Softmax(dim=1),\n",
    "    \"LeakyReLU\": lambda slope=0.01, *_: nn.LeakyReLU(negative_slope=float(slope)),\n",
    "    \"GELU\": lambda *_: nn.GELU(),\n",
    "    \"ELU\": lambda alpha=1.0, *_: nn.ELU(alpha=float(alpha))\n",
    "}\n",
    "\n",
    "def parse_int_or_tuple(val):\n",
    "    try:\n",
    "        return tuple(map(int, str(val).split(','))) if ',' in str(val) else int(val)\n",
    "    except Exception:\n",
    "        raise ValueError(f\"Invalid numeric input: '{val}'. Please enter an integer or comma-separated pair.\")\n",
    "\n",
    "layer_configs = []\n",
    "\n",
    "def validate_layer_inputs(layer_type, **kwargs):\n",
    "    try:\n",
    "        if layer_type == \"Linear\":\n",
    "            in_dim_int = int(kwargs.get(\"in_dim\"))\n",
    "            out_dim_int = int(kwargs.get(\"out_dim\"))\n",
    "            if in_dim_int <= 0 or out_dim_int <= 0:\n",
    "                return False, f\"{layer_type} dimensions must be positive integers\"\n",
    "\n",
    "        elif layer_type == \"Conv2d\":\n",
    "            in_dim_int = int(kwargs.get(\"in_dim\"))\n",
    "            out_dim_int = int(kwargs.get(\"out_dim\"))\n",
    "            kernel_dim = parse_int_or_tuple(kwargs.get(\"kernel_size\", 3))\n",
    "            padding_dim = parse_int_or_tuple(kwargs.get(\"padding\", 1))\n",
    "            stride = parse_int_or_tuple(kwargs.get(\"stride\", 1))\n",
    "\n",
    "            for val in [kernel_dim, padding_dim, stride]:\n",
    "                if isinstance(val, tuple):\n",
    "                    if any(v < 0 for v in val):\n",
    "                        return False, f\"{layer_type} tuple values must be non-negative\"\n",
    "                else:\n",
    "                    if val < 0:\n",
    "                        return False, f\"{layer_type} values must be non-negative\"\n",
    "\n",
    "            if in_dim_int <= 0 or out_dim_int <= 0:\n",
    "                return False, f\"{layer_type} in/out dims must be positive integers\"\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(kwargs.get(\"in_dim\"))\n",
    "            if not (0 <= p <= 1):\n",
    "                return False, \"Dropout probability must be between 0 and 1\"\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kernel = parse_int_or_tuple(kwargs.get(\"pool_kernel\", 2))\n",
    "            stride = parse_int_or_tuple(kwargs.get(\"pool_stride\", 2))\n",
    "            padding = parse_int_or_tuple(kwargs.get(\"pool_padding\", 0))\n",
    "            for val in [kernel, stride, padding]:\n",
    "                if isinstance(val, tuple):\n",
    "                    if any(v < 0 for v in val):\n",
    "                        return False, f\"{layer_type} tuple values must be non-negative\"\n",
    "                else:\n",
    "                    if val < 0:\n",
    "                        return False, f\"{layer_type} values must be non-negative\"\n",
    "        \n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kernel = parse_int_or_tuple(kwargs.get(\"avgpool_kernel\", 2))\n",
    "            stride = parse_int_or_tuple(kwargs.get(\"avgpool_stride\", 2))\n",
    "            padding = parse_int_or_tuple(kwargs.get(\"avgpool_padding\", 0))\n",
    "            for val in [kernel, stride, padding]:\n",
    "                if isinstance(val, tuple):\n",
    "                    if any(v < 0 for v in val):\n",
    "                        return False, f\"{layer_type} tuple values must be non-negative\"\n",
    "                else:\n",
    "                    if val < 0:\n",
    "                        return False, f\"{layer_type} values must be non-negative\"\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            slope = float(kwargs.get(\"leaky_slope\", 0.01))\n",
    "            if slope < 0:\n",
    "                return False, \"LeakyReLU negative_slope must be ‚â• 0\"\n",
    "        \n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(kwargs.get(\"elu_alpha\", 1.0))\n",
    "            if alpha < 0:\n",
    "                return False, \"ELU alpha must be ‚â• 0\"\n",
    "\n",
    "        return True, None\n",
    "    except Exception as e:\n",
    "        return False, f\"Validation error in {layer_type}: {str(e)}\"\n",
    "\n",
    "def add_layer(\n",
    "    layer_type, in_dim, out_dim,\n",
    "    kernel_size=3, padding=1, stride=1, bias=1,\n",
    "    pool_kernel=\"2\", pool_stride=\"2\", pool_padding=\"0\",\n",
    "    avgpool_kernel=None, avgpool_stride=None, avgpool_padding=None,\n",
    "    leaky_slope = \"0.01\", elu_alpha = \"1.0\"\n",
    "):\n",
    "\n",
    "    is_valid, err_msg = validate_layer_inputs(\n",
    "        layer_type=layer_type,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        pool_stride=pool_stride,\n",
    "        pool_padding=pool_padding,\n",
    "        avgpool_kernel=avgpool_kernel, \n",
    "        avgpool_stride=avgpool_stride,\n",
    "        avgpool_padding=avgpool_padding,\n",
    "        leaky_slope=leaky_slope,\n",
    "        elu_alpha=elu_alpha\n",
    "    )\n",
    "\n",
    "    if not is_valid:\n",
    "        return err_msg\n",
    "\n",
    "    try:\n",
    "        if layer_type == \"Conv2d\":\n",
    "            in_dim = int(in_dim)\n",
    "            out_dim = int(out_dim)\n",
    "            k = parse_int_or_tuple(kernel_size or 3)\n",
    "            p = parse_int_or_tuple(padding or 1)\n",
    "            s = parse_int_or_tuple(stride or 1)\n",
    "            b = bool(bias)\n",
    "            desc = f\"Conv2d({in_dim}, {out_dim}, kernel={k}, padding={p}, stride={s}, bias={b})\"\n",
    "            config = (desc, layer_type, in_dim, out_dim, k, p, s, b)\n",
    "        \n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            negative_slope = float(leaky_slope or \"0.01\")\n",
    "            desc = f\"LeakyReLU(negative_slope={negative_slope})\"\n",
    "            config = (desc, layer_type, negative_slope, negative_slope, None, None, None, None)\n",
    "        \n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(elu_alpha or \"1.0\")\n",
    "            desc = f\"ELU(alpha={alpha})\"\n",
    "            config = (desc, layer_type, alpha, alpha, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Softmax\":\n",
    "            desc = \"Softmax(dim=1)\"\n",
    "            config = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"GELU\":\n",
    "            desc = \"GELU()\"\n",
    "            config = (desc, layer_type, None, None, None, None, None, None)\n",
    "            \n",
    "        elif layer_type == \"Linear\":\n",
    "            in_dim = int(in_dim)\n",
    "            out_dim = int(out_dim)\n",
    "            desc = f\"Linear({in_dim}, {out_dim})\"\n",
    "            config = (desc, layer_type, in_dim, out_dim, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(in_dim)\n",
    "            desc = f\"Dropout({p})\"\n",
    "            config = (desc, layer_type, p, p, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kernel_val = parse_int_or_tuple(pool_kernel or \"2\")\n",
    "            stride_val = parse_int_or_tuple(pool_stride or \"2\")\n",
    "            padding_val = parse_int_or_tuple(pool_padding or \"0\")\n",
    "            desc = f\"MaxPool2d(kernel={kernel_val}, stride={stride_val}, padding={padding_val})\"\n",
    "            config = (desc, layer_type, None, None, kernel_val, padding_val, stride_val, None)\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kernel_val = parse_int_or_tuple(avgpool_kernel or \"2\")\n",
    "            stride_val = parse_int_or_tuple(avgpool_stride or \"2\")\n",
    "            padding_val = parse_int_or_tuple(avgpool_padding or \"0\")\n",
    "            desc = f\"AvgPool2d(kernel={kernel_val}, stride={stride_val}, padding={padding_val})\"\n",
    "            config = (desc, layer_type, None, None, kernel_val, padding_val, stride_val, None)\n",
    "\n",
    "        else:\n",
    "            # For e.g. ReLU/Tanh/Sigmoid/Flatten\n",
    "            desc = layer_type\n",
    "            config = (desc, layer_type, None, None, None, None, None, None)\n",
    "            \n",
    "    except Exception as e:\n",
    "        desc = f\"[Error Adding Layer: {e}]\"\n",
    "        config = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "    layer_configs.append(config)\n",
    "    return update_architecture_text()\n",
    "\n",
    "def update_layer(\n",
    "    index, layer_type, in_dim, out_dim,\n",
    "    kernel_size=3, padding=1, stride=1, bias=True,\n",
    "    pool_kernel=\"2\", pool_stride=\"2\", pool_padding=\"0\",\n",
    "    avgpool_kernel=None, avgpool_stride=None, avgpool_padding=None,\n",
    "    leaky_slope= \"0.01\",elu_alpha = \"1.0\"\n",
    "):\n",
    "    \n",
    "    index = int(index)\n",
    "    is_valid, err_msg = validate_layer_inputs(\n",
    "        layer_type=layer_type,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        pool_stride=pool_stride,\n",
    "        pool_padding=pool_padding,\n",
    "        avgpool_kernel=avgpool_kernel, \n",
    "        avgpool_stride=avgpool_stride,\n",
    "        avgpool_padding=avgpool_padding,\n",
    "        leaky_slope=leaky_slope,\n",
    "        elu_alpha=elu_alpha\n",
    "    )\n",
    "\n",
    "    if not is_valid:\n",
    "        return err_msg\n",
    "    if index < 0 or index >= len(layer_configs):\n",
    "        return update_architecture_text()\n",
    "\n",
    "    try:\n",
    "        if layer_type == \"Conv2d\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            k = parse_int_or_tuple(kernel_size or 3)\n",
    "            p = parse_int_or_tuple(padding or 1)\n",
    "            s = parse_int_or_tuple(stride or 1)\n",
    "            b = bool(bias)\n",
    "            desc = f\"Conv2d({i}, {o}, kernel={k}, padding={p}, stride={s}, bias={b})\"\n",
    "            layer_configs[index] = (desc, layer_type, i, o, k, p, s, b)\n",
    "\n",
    "        elif layer_type == \"Linear\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            desc = f\"Linear({i}, {o})\"\n",
    "            layer_configs[index] = (desc, layer_type, i, o, None, None, None, None)\n",
    "        \n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(elu_alpha or \"1.0\")\n",
    "            desc = f\"ELU(alpha={alpha})\"\n",
    "            layer_configs[index] = (desc, layer_type, alpha, alpha, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"GELU\":\n",
    "            desc = \"GELU()\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            negative_slope = float(leaky_slope or \"0.01\")\n",
    "            desc = f\"LeakyReLU(negative_slope={negative_slope})\"\n",
    "            layer_configs[index] = (desc, layer_type, negative_slope, negative_slope, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Softmax\":\n",
    "            desc = \"Softmax(dim=1)\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(in_dim)\n",
    "            desc = f\"Dropout({p})\"\n",
    "            layer_configs[index] = (desc, layer_type, p, p, None, None, None, None)\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kv = parse_int_or_tuple(avgpool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(avgpool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(avgpool_padding or \"0\")\n",
    "            desc = f\"AvgPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, kv, pv, sv, None)\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kv = parse_int_or_tuple(pool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(pool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(pool_padding or \"0\")\n",
    "            desc = f\"MaxPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs[index] = (desc, layer_type, None, None, kv, pv, sv, None)\n",
    "\n",
    "        else:\n",
    "            desc = layer_type\n",
    "            layer_configs[index] = (desc, layer_type, None, None, None, None, None, None)\n",
    "\n",
    "    except Exception as e:\n",
    "        layer_configs[index] = (f\"[Error Editing Layer: {e}]\", layer_type, None, None, None, None, None, None)\n",
    "\n",
    "    return update_architecture_text()\n",
    "\n",
    "def insert_layer(\n",
    "    index, layer_type, in_dim, out_dim,\n",
    "    kernel_size=3, padding=1, stride=1, bias=1,\n",
    "    pool_kernel=\"2\", pool_stride=\"2\", pool_padding=\"0\",\n",
    "    avgpool_kernel=None, avgpool_stride=None, avgpool_padding=None, \n",
    "    leaky_slope=\"0.01\", elu_alpha = \"1.0\"\n",
    "):\n",
    "    index = int(index)\n",
    "    is_valid, err_msg = validate_layer_inputs(\n",
    "        layer_type=layer_type,\n",
    "        in_dim=in_dim,\n",
    "        out_dim=out_dim,\n",
    "        kernel_size=kernel_size,\n",
    "        padding=padding,\n",
    "        stride=stride,\n",
    "        pool_kernel=pool_kernel,\n",
    "        pool_stride=pool_stride,\n",
    "        pool_padding=pool_padding,\n",
    "        avgpool_kernel=avgpool_kernel, \n",
    "        avgpool_stride=avgpool_stride,\n",
    "        avgpool_padding=avgpool_padding,\n",
    "        leaky_slope=leaky_slope,\n",
    "        elu_alpha=elu_alpha\n",
    "    )\n",
    "\n",
    "    if not is_valid:\n",
    "        return err_msg\n",
    "\n",
    "    try:\n",
    "        if layer_type == \"Conv2d\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            k = parse_int_or_tuple(kernel_size or \"3\")\n",
    "            p = parse_int_or_tuple(padding or \"1\")\n",
    "            s = parse_int_or_tuple(stride or \"1\")\n",
    "            b = bool(bias)\n",
    "            desc = f\"Conv2d({i}, {o}, kernel={k}, padding={p}, stride={s}, bias={b})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, i, o, k, p, s, b))\n",
    "\n",
    "        elif layer_type == \"Linear\":\n",
    "            i = int(in_dim)\n",
    "            o = int(out_dim)\n",
    "            desc = f\"Linear({i}, {o})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, i, o, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"ELU\":\n",
    "            alpha = float(elu_alpha or \"1.0\")\n",
    "            desc = f\"ELU(alpha={alpha})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, alpha, alpha, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            negative_slope = float(leaky_slope or \"0.01\")\n",
    "            desc = f\"LeakyReLU(negative_slope={negative_slope})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, negative_slope, negative_slope, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"Softmax\":\n",
    "            desc = \"Softmax(dim=1)\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            p = float(in_dim)\n",
    "            desc = f\"Dropout({p})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, p, p, None, None, None, None))\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            kv = parse_int_or_tuple(pool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(pool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(pool_padding or \"0\")\n",
    "            desc = f\"MaxPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, kv, pv, sv, None))\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            kv = parse_int_or_tuple(avgpool_kernel or \"2\")\n",
    "            sv = parse_int_or_tuple(avgpool_stride or \"2\")\n",
    "            pv = parse_int_or_tuple(avgpool_padding or \"0\")\n",
    "            desc = f\"AvgPool2d(kernel={kv}, stride={sv}, padding={pv})\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, kv, pv, sv, None))\n",
    "\n",
    "        elif layer_type == \"GELU\":\n",
    "            desc = \"GELU()\"\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "        else:\n",
    "            desc = layer_type\n",
    "            layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "    except Exception as e:\n",
    "        desc = f\"[Error Inserting Layer: {e}]\"\n",
    "        layer_configs.insert(index, (desc, layer_type, None, None, None, None, None, None))\n",
    "\n",
    "    return update_architecture_text()\n",
    "\n",
    "def delete_layer(index):\n",
    "    index = int(index)\n",
    "    if 0 <= index < len(layer_configs):\n",
    "        layer_configs.pop(index)\n",
    "    return update_architecture_text()\n",
    "\n",
    "def reset_layers():\n",
    "    layer_configs.clear()\n",
    "    return \"\"\n",
    "\n",
    "def update_architecture_text(highlight_index=None):\n",
    "    lines = []\n",
    "    for i, config in enumerate(layer_configs):\n",
    "        prefix = f\"{i}: \"\n",
    "        desc = config[0]\n",
    "        if i == highlight_index:\n",
    "            desc = f\"‚ö†Ô∏è {desc}\"\n",
    "        lines.append(prefix + desc)\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_model():\n",
    "    layers = []\n",
    "    for config in layer_configs:\n",
    "        _, layer_type, in_dim, out_dim, kernel, padding, stride, bias = config\n",
    "\n",
    "        if layer_type == \"Conv2d\":\n",
    "            layers.append(nn.Conv2d(\n",
    "                int(in_dim), int(out_dim),\n",
    "                kernel_size=kernel,\n",
    "                padding=padding,\n",
    "                stride=stride,\n",
    "                bias=bool(bias)\n",
    "            ))\n",
    "\n",
    "        elif layer_type == \"Linear\":\n",
    "            layers.append(nn.Linear(int(in_dim), int(out_dim)))\n",
    "        \n",
    "        elif layer_type == \"GELU\":\n",
    "            layers.append(nn.GELU())\n",
    "\n",
    "        elif layer_type == \"LeakyReLU\":\n",
    "            layers.append(nn.LeakyReLU(negative_slope=float(in_dim)))\n",
    "\n",
    "        elif layer_type == \"ELU\":\n",
    "            layers.append(nn.ELU(alpha=float(in_dim)))\n",
    "\n",
    "        elif layer_type == \"Dropout\":\n",
    "            layers.append(nn.Dropout(float(in_dim)))\n",
    "        \n",
    "        elif layer_type == \"Softmax\":\n",
    "            layers.append(nn.Softmax(dim=1))\n",
    "\n",
    "        elif layer_type == \"MaxPool2d\":\n",
    "            layers.append(nn.MaxPool2d(kernel_size=kernel, stride=stride, padding=padding))\n",
    "\n",
    "        elif layer_type == \"AvgPool2d\":\n",
    "            layers.append(nn.AvgPool2d(kernel_size=kernel, stride=stride, padding=padding))\n",
    "\n",
    "        else:\n",
    "            # e.g. ReLU, Tanh, Sigmoid, Flatten\n",
    "            layers.append(layer_map[layer_type]())\n",
    "\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def extract_zip_to_tempdir(file_like, custom_path=None):\n",
    "\n",
    "    if custom_path:\n",
    "        os.makedirs(custom_path, exist_ok=True)\n",
    "        temp_dir = tempfile.mkdtemp(dir=custom_path)\n",
    "    else:\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "    with zipfile.ZipFile(file_like, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "    return temp_dir\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def safe_pil_loader(path, num_channels=3):\n",
    "    try:\n",
    "        with open(path, 'rb') as f:\n",
    "            img = Image.open(f)\n",
    "            img = img.convert('RGB' if num_channels == 3 else 'L')\n",
    "            img.load()\n",
    "            return img\n",
    "    except Exception as e:\n",
    "        print(f\"Skipping corrupted image: {path} ‚Äî {e}\")\n",
    "        return None\n",
    "\n",
    "class SafeImageFolder(datasets.ImageFolder):\n",
    "    def __init__(self, root, transform=None, num_channels=3):\n",
    "        # Create a lambda that captures `num_channels`\n",
    "        loader_with_channels = lambda path: safe_pil_loader(path, num_channels)\n",
    "        super().__init__(root, transform=transform, loader=loader_with_channels)\n",
    "        \n",
    "        before = len(self.samples)\n",
    "        self.samples = [s for s in self.samples if loader_with_channels(s[0]) is not None]\n",
    "        after = len(self.samples)\n",
    "        self.imgs = self.samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        img = self.loader(path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Could not load image at {path}\")\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "        return img, target\n",
    "\n",
    "def load_data(file, custom_path=None, batch_size=32, image_size=28,  num_channels=3):\n",
    "    ext = os.path.splitext(file)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        df = pd.read_csv(file)\n",
    "        if 'y' not in df.columns:\n",
    "            raise ValueError(\"CSV must contain a 'y' column.\")\n",
    "        X = df.drop(columns=['y']).values\n",
    "        y = df['y'].values\n",
    "        X, y = shuffle(X, y)\n",
    "        return {\n",
    "            \"type\": \"tabular\",\n",
    "            \"train\": (X, y),\n",
    "            \"path\": None\n",
    "        }\n",
    "    elif ext == \".zip\":\n",
    "        with open(file, 'rb') as f:\n",
    "            data_dir = extract_zip_to_tempdir(f, custom_path)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((image_size, image_size)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        dataset = SafeImageFolder(data_dir, transform=transform, num_channels=num_channels)\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "        return {\n",
    "            \"type\": \"image\",\n",
    "            \"train\": train_loader,\n",
    "            \"path\": data_dir\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Provide a .csv or .zip path.\")\n",
    "\n",
    "def get_flat_weights(model):\n",
    "    return torch.cat([p.detach().flatten() for p in model.parameters()])\n",
    "\n",
    "def generate_loss_plot(loss_history):\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(loss_history)\n",
    "    ax.set_title(\"Loss over Epochs\")\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(\"Loss\")\n",
    "    tmpfile = tempfile.NamedTemporaryFile(suffix=\".png\", delete=False)\n",
    "    plt.savefig(tmpfile.name)\n",
    "    tmpfile.close()\n",
    "    plt.close(fig)\n",
    "    return tmpfile.name\n",
    "\n",
    "def generate_3d_animation_pca(weight_path, loss_history, output_path):\n",
    "    \"\"\"\n",
    "    weight_path: Numpy array of flattened weights over training steps\n",
    "    loss_history: List of loss values\n",
    "    output_path:  Where the final .mp4 animation should be saved\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Do PCA\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced = pca.fit_transform(weight_path)\n",
    "\n",
    "    # 2) Make the 3D figure\n",
    "    fig = plt.figure(figsize=(10, 7))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    ax.plot(reduced[:, 0], reduced[:, 1], loss_history, color='red')\n",
    "    point, = ax.plot([reduced[0, 0]], [reduced[0, 1]], [loss_history[0]], 'ro')\n",
    "\n",
    "    def update(i):\n",
    "        point.set_data([reduced[i, 0]], [reduced[i, 1]])\n",
    "        point.set_3d_properties([loss_history[i]])\n",
    "        return point,\n",
    "\n",
    "    ani = animation.FuncAnimation(\n",
    "        fig, update,\n",
    "        frames=len(loss_history),\n",
    "        interval=100,\n",
    "        blit=True\n",
    "    )\n",
    "\n",
    "    # 3) Write an intermediate .gif in the same folder as output_path\n",
    "    #    for example, if output_path=\"outputs/animation.mp4\",\n",
    "    #    we can do something like \"outputs/animation_temp.gif\"\n",
    "    base, _ = os.path.splitext(output_path)\n",
    "    gif_path = f\"{base}_temp.gif\"\n",
    "\n",
    "    # Save as a GIF\n",
    "    ani.save(gif_path, writer='pillow')\n",
    "    plt.close(fig)\n",
    "\n",
    "    # 4) Convert .gif to .mp4\n",
    "    clip = VideoFileClip(gif_path)\n",
    "    clip.write_videofile(output_path, codec='libx264')\n",
    "    clip.close()\n",
    "\n",
    "    # 5) Remove the temp .gif\n",
    "    os.remove(gif_path)\n",
    "\n",
    "    # 6) Return the final MP4 path\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def validate_model_forward_pass(model, data_type, image_size=28, num_features=None, num_channels=3):\n",
    "    try:\n",
    "        if data_type == \"tabular\":\n",
    "            assert num_features is not None, \"Missing number of input features for tabular data\"\n",
    "            dummy_input = torch.randn(1, num_features).to(device)\n",
    "        else:\n",
    "            dummy_input = torch.randn(1, num_channels, image_size, image_size).to(device)\n",
    "\n",
    "        x = dummy_input\n",
    "        for idx, layer in enumerate(model):\n",
    "            try:\n",
    "                x = layer(x)\n",
    "            except Exception as e:\n",
    "                return False, f\"Shape mismatch at layer {idx}: {layer.__class__.__name__} ‚Äî {str(e)}\", idx\n",
    "        return True, None, None\n",
    "    except Exception as e:\n",
    "        return False, f\"Unexpected validation error: {str(e)}\", None\n",
    "\n",
    "def train_model(loss_name, opt_name, lr, batch_size='32', image_size='28', file=None, custom_path=None, epochs='100', num_channels=3, generate_animation=False):\n",
    "    \"\"\"\n",
    "    Must always return exactly 5 items to match the 5 outputs in trainer_interface!\n",
    "    We'll now use yield so we can stream partial updates:\n",
    "      (loss_plot, animation_path, model_path, architecture_text, log_text)\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    animation_path = None  # ‚úÖ safe default\n",
    "    loss_plot_path = None\n",
    "    model_path = None\n",
    "    final_logs = \"\"\n",
    "    try:\n",
    "        channels = int(num_channels)\n",
    "        if channels not in [1, 3]:\n",
    "            yield None, None, None, update_architecture_text(), \"‚ùå Channels must be 1 or 3.\"\n",
    "            return\n",
    "    except:\n",
    "        yield None, None, None, update_architecture_text(), \"‚ùå Channels must be numeric.\"\n",
    "        return\n",
    "\n",
    "    # 1) Convert epochs to int with early validation\n",
    "    try:\n",
    "        max_epochs = int(epochs)\n",
    "        if max_epochs <= 0:\n",
    "            # Immediately yield error and stop\n",
    "            yield None, None, None, update_architecture_text(), \"‚ùå Epochs must be a positive integer.\"\n",
    "            return\n",
    "    except:\n",
    "        yield None, None, None, update_architecture_text(), \"‚ùå Epochs must be a valid number.\"\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # 2) If no model is configured\n",
    "        if not layer_configs:\n",
    "            yield None, None, None, update_architecture_text(), \"‚ùå No model configured! Please add at least one trainable layer.\"\n",
    "            return\n",
    "\n",
    "        # 3) If file path is invalid\n",
    "        if not os.path.exists(file):\n",
    "            msg = f\"‚ùå File not found: {file}\\nPlease check the path and try again.\"\n",
    "            yield None, None, None, update_architecture_text(), msg\n",
    "            return\n",
    "\n",
    "        # 4) If file type is not .csv or .zip\n",
    "        if not (file.endswith('.csv') or file.endswith('.zip')):\n",
    "            yield None, None, None, update_architecture_text(), \"‚ùå Invalid file type. Please provide a .csv or .zip file.\"\n",
    "            return\n",
    "\n",
    "        # 5) If custom directory is invalid\n",
    "        if custom_path and not os.path.isdir(custom_path):\n",
    "            try:\n",
    "                os.makedirs(custom_path, exist_ok=True)\n",
    "            except Exception as e:\n",
    "                msg = f\"‚ùå Could not create directory '{custom_path}': {e}\"\n",
    "                yield None, None, None, update_architecture_text(), msg\n",
    "                return\n",
    "\n",
    "        # 6) Build model\n",
    "        lr = float(lr)\n",
    "        model = build_model().to(device)\n",
    "\n",
    "        # 7) Validate forward pass\n",
    "        if file.endswith(\".csv\"):\n",
    "            df = pd.read_csv(file)\n",
    "            if 'y' not in df.columns:\n",
    "                yield None, None, None, update_architecture_text(), \"‚ùå CSV missing 'y' column.\"\n",
    "                return\n",
    "            num_features = df.shape[1] - 1\n",
    "            is_valid, error_msg, bad_layer_idx = validate_model_forward_pass(model, \"tabular\", num_features=num_features)\n",
    "        else:\n",
    "            is_valid, error_msg, bad_layer_idx = validate_model_forward_pass(model, \"image\", image_size=int(image_size), num_channels=num_channels)\n",
    "\n",
    "        if not is_valid:\n",
    "            # highlight offending layer + yield\n",
    "            updated_view = update_architecture_text(highlight_index=bad_layer_idx)\n",
    "            yield None, None, None, updated_view, \"\"\n",
    "            return\n",
    "\n",
    "        # 8) Check for trainable params\n",
    "        if not any(p.requires_grad for p in model.parameters()):\n",
    "            yield None, None, None, update_architecture_text(), \"‚ö†Ô∏è Model has no trainable parameters. Add a Linear or Conv2d layer.\"\n",
    "            return\n",
    "\n",
    "        # 9) Validate batch_size\n",
    "        try:\n",
    "            batch_size = int(batch_size)\n",
    "            if batch_size <= 0:\n",
    "                yield None, None, None, update_architecture_text(), \"‚ùå Batch size must be a positive integer\"\n",
    "                return\n",
    "        except:\n",
    "            yield None, None, None, update_architecture_text(), \"‚ùå Batch size must be a valid number\"\n",
    "            return\n",
    "\n",
    "        # 10) Validate image_size\n",
    "        try:\n",
    "            image_size = int(image_size)\n",
    "            if image_size <= 0:\n",
    "                yield None, None, None, update_architecture_text(), \"‚ùå Image size must be a positive integer\"\n",
    "                return\n",
    "        except:\n",
    "            yield None, None, None, update_architecture_text(), \"‚ùå Image size must be a valid number\"\n",
    "            return\n",
    "\n",
    "        # 11) Load data\n",
    "        data = load_data(file, custom_path, batch_size=batch_size, image_size=image_size, num_channels=channels)\n",
    "        loss_fn = nn.MSELoss() if loss_name == 'MSELoss' else nn.CrossEntropyLoss()\n",
    "        optimizer = optim.SGD(model.parameters(), lr=lr) if opt_name == 'SGD' else optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "        loss_history = []\n",
    "        weight_path = []\n",
    "        status_lines = []\n",
    "\n",
    "\n",
    "        print(f\"[DEBUG] Start of train_model: len(loss_history) = {len(loss_history)}\")\n",
    "\n",
    "\n",
    "        # 12) Train\n",
    "\n",
    "        from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "        if data[\"type\"] == \"tabular\":\n",
    "            X_train, y_train = data[\"train\"]\n",
    "            X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "            y_train = torch.tensor(y_train, dtype=torch.long)\n",
    "        \n",
    "            train_dataset = TensorDataset(X_train, y_train)\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        \n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                epoch_loss = 0\n",
    "                num_batches = 0\n",
    "        \n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "        \n",
    "                    optimizer.zero_grad()\n",
    "        \n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        y_input = torch.nn.functional.one_hot(y_batch, num_classes=2).float()\n",
    "                    else:\n",
    "                        y_input = y_batch\n",
    "        \n",
    "                    out = model(X_batch)\n",
    "        \n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        out = torch.softmax(out, dim=1)\n",
    "        \n",
    "                    loss = loss_fn(out, y_input)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "        \n",
    "                    epoch_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "                    weight_path.append(get_flat_weights(model).cpu().numpy())\n",
    "        \n",
    "                avg_epoch_loss = epoch_loss / num_batches\n",
    "                loss_history.append(avg_epoch_loss)\n",
    "        \n",
    "                status_lines.append(f\"Epoch {epoch}/{max_epochs} ‚Äî Loss: {avg_epoch_loss:.4f}\")\n",
    "                yield None, None, None, update_architecture_text(), \"\\n\\n\".join(status_lines)\n",
    "\n",
    "        else:\n",
    "            # image data\n",
    "            train_loader = data[\"train\"]\n",
    "            for epoch in range(1, max_epochs + 1):\n",
    "                epoch_loss = 0\n",
    "                num_batches = 0\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    y_batch = y_batch.to(device)\n",
    "\n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        # One-hot for MSE\n",
    "                        y_input = torch.nn.functional.one_hot(\n",
    "                            y_batch,\n",
    "                            num_classes=len(torch.unique(y_batch))\n",
    "                        ).float().to(device)\n",
    "                    else:\n",
    "                        y_input = y_batch\n",
    "\n",
    "                    out = model(X_batch)\n",
    "                    if isinstance(loss_fn, nn.MSELoss):\n",
    "                        out = torch.softmax(out, dim=1)\n",
    "\n",
    "                    loss = loss_fn(out, y_input)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss += loss.item()\n",
    "                    num_batches += 1\n",
    "\n",
    "                    weight_path.append(get_flat_weights(model).cpu().numpy())\n",
    "\n",
    "\n",
    "    # Average loss for the epoch\n",
    "                avg_epoch_loss = epoch_loss / num_batches\n",
    "                loss_history.append(avg_epoch_loss)\n",
    "\n",
    "                status_lines.append(f\"Epoch {epoch}/{max_epochs} ‚Äî Loss: {avg_epoch_loss:.4f}\")\n",
    "                yield None, None, None, update_architecture_text(), \"\\n\\n\".join(status_lines)\n",
    "\n",
    "            # Cleanup extracted images\n",
    "            if data[\"path\"]:\n",
    "                try:\n",
    "                    shutil.rmtree(data[\"path\"])\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not delete extracted folder: {e}\")\n",
    "\n",
    "        # 13) Produce final outputs (loss plot, 3D animation, model file, architecture, logs)\n",
    "        print(f\"[DEBUG] After training loop: len(loss_history) = {len(loss_history)}\")\n",
    "\n",
    "        loss_plot_path = os.path.join(OUTPUT_DIR, \"loss_plot.png\")\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.plot(loss_history)\n",
    "        ax.set_title(\"Loss over Epochs\")\n",
    "        ax.set_xlabel(\"Epoch\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "        plt.savefig(loss_plot_path)\n",
    "        plt.close(fig)\n",
    "        animation_path = os.path.join(OUTPUT_DIR, \"animation.mp4\")\n",
    "        \n",
    "        if generate_animation:\n",
    "\n",
    "            generate_3d_animation_pca(\n",
    "            np.array(weight_path),\n",
    "            loss_history,\n",
    "            animation_path\n",
    "            )\n",
    "        else:\n",
    "            create_dummy_video(animation_path)\n",
    "# Save the trained model in OUTPUT_DIR\n",
    "        model_path = os.path.join(OUTPUT_DIR, \"trained_model.pt\")\n",
    "        torch.save(model.state_dict(), model_path)\n",
    "\n",
    "        final_logs = \"\\n\".join(status_lines)\n",
    "\n",
    "        # The final yield is still 5 items\n",
    "\n",
    "        print(f\"[DEBUG] Final yield:\")\n",
    "        print(f\" - Loss plot: {loss_plot_path}\")\n",
    "        print(f\" - Animation path: {animation_path}\")\n",
    "        print(f\" - Model path: {model_path}\")\n",
    "        print(f\" - Arch text: {update_architecture_text()}\")\n",
    "        print(f\" - Logs: {final_logs}\")\n",
    "\n",
    "        for path in [loss_plot_path, animation_path, model_path]:\n",
    "            if os.path.isdir(path):\n",
    "                raise ValueError(f\"‚ùå Output path is a directory, expected a file: {path}\")\n",
    "        print(f\"[DEBUG] Before final yield: len(loss_history) = {len(loss_history)}\")\n",
    "\n",
    "        yield loss_plot_path,  animation_path, model_path, update_architecture_text(), final_logs\n",
    "\n",
    "    except gr.Error as e:\n",
    "        raise e\n",
    "    except Exception as e:\n",
    "        raise gr.Error(f\"‚ùå Unexpected error: {str(e)}\")\n",
    "\n",
    "def get_device_status():\n",
    "    if torch.cuda.is_available():\n",
    "        return f\"üü¢ GPU: {torch.cuda.get_device_name(0)}\"\n",
    "    else:\n",
    "        return f\"üî¥ CPU: {cpuinfo.get_cpu_info()}\"\n",
    "\n",
    "\n",
    "def get_default_writable_folder():\n",
    "    \"\"\"\n",
    "    Returns a subfolder in the user's home directory, e.g.:\n",
    "      Windows: C:\\\\Users\\\\<USERNAME>\\\\my_gradio_data\n",
    "      Linux/Mac: /home/<USERNAME>/my_gradio_data\n",
    "    This avoids writing to the root of C: or system-protected paths.\n",
    "    \"\"\"\n",
    "    home_dir = os.path.expanduser(\"~\")  # e.g. C:\\\\Users\\\\Alice on Windows\n",
    "    default_path = os.path.join(home_dir, \"my_gradio_data\")\n",
    "    os.makedirs(default_path, exist_ok=True)\n",
    "    return default_path\n",
    "\n",
    "\n",
    "def train_model_with_default_path(\n",
    "    loss_name, opt_name, lr, batch_size, image_size,\n",
    "    file, custom_path, epochs, num_channels, generate_animation\n",
    "):\n",
    "    if not custom_path or custom_path.strip() == \"\":\n",
    "        custom_path = get_default_writable_folder()\n",
    "\n",
    "    # Instead of returning the generator, re-yield its contents:\n",
    "    for item in train_model(\n",
    "        loss_name, opt_name, lr, batch_size, image_size,\n",
    "        file, custom_path, epochs, num_channels, generate_animation\n",
    "    ):\n",
    "        yield item\n",
    "\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def create_dummy_video(output_path):\n",
    "    command = [\n",
    "        \"ffmpeg\",\n",
    "        \"-f\", \"lavfi\",\n",
    "        \"-i\", \"color=c=black:s=1280x720:d=5\",  # ‚¨ÖÔ∏è bump resolution and duration\n",
    "        \"-c:v\", \"libx264\",\n",
    "        \"-t\", \"5\",\n",
    "        \"-pix_fmt\", \"yuv420p\",\n",
    "        \"-y\", output_path\n",
    "    ]\n",
    "    try:\n",
    "        result = subprocess.run(command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        print(f\"‚úÖ Dummy video created at: {output_path}\")\n",
    "        print(result.stdout.decode())\n",
    "        print(result.stderr.decode())\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"‚ùå Error creating dummy video: {e.stderr.decode()}\")\n",
    "\n",
    "# --------------------------------\n",
    "# Gradio wiring\n",
    "# --------------------------------\n",
    "\n",
    "with gr.Blocks() as dashboard:\n",
    "\n",
    "    with gr.Tab(\"Build Model\"):\n",
    "        gr.Markdown(\"## Build a Layer\")\n",
    "        builder_arch = gr.Textbox(label=\"Architecture So Far\", lines=6)\n",
    "\n",
    "        layer_type_dropdown = gr.Dropdown(choices=list(layer_map.keys()), label=\"Layer Type\", value=\"Linear\")\n",
    "        in_dim = gr.Textbox(label=\"Input Dim (Linear/Conv2d)\")\n",
    "        out_dim = gr.Textbox(label=\"Output Dim (Linear/Conv2d)\")\n",
    "\n",
    "        conv_kernel = gr.Textbox(label=\"Kernel Size\", value=\"3\", visible=False)\n",
    "        conv_padding = gr.Textbox(label=\"Padding\", value=\"1\", visible=False)\n",
    "        conv_stride = gr.Textbox(label=\"Stride\", value=\"1\", visible=False)\n",
    "        conv_bias = gr.Checkbox(label=\"Include Bias\", value=True, visible=False)\n",
    "\n",
    "        pool_kernel = gr.Textbox(label=\"Pool Kernel Size\", value=\"2\", visible=False)\n",
    "        pool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        pool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        avgpool_kernel = gr.Textbox(label=\"AvgPool Kernel Size\", value=\"2\", visible=False)\n",
    "        avgpool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        avgpool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        leaky_relu_slope = gr.Textbox(label=\"Negative Slope\", value=\"0.01\", visible=False)\n",
    "        elu_alpha = gr.Textbox(label=\"ELU Alpha\", value=\"1.0\", visible=False)\n",
    "\n",
    "        add_btn = gr.Button(\"Add Layer\")\n",
    "        add_btn.click(\n",
    "            fn=add_layer,\n",
    "            inputs=[\n",
    "                layer_type_dropdown, in_dim, out_dim,\n",
    "                conv_kernel, conv_padding, conv_stride, conv_bias,\n",
    "                pool_kernel, pool_stride, pool_padding,\n",
    "                avgpool_kernel, avgpool_stride, avgpool_padding,\n",
    "                leaky_relu_slope, elu_alpha\n",
    "            ],\n",
    "            outputs=builder_arch\n",
    "        )\n",
    "\n",
    "        def toggle_fields(layer_type):\n",
    "            is_conv = (layer_type == \"Conv2d\")\n",
    "            is_pool = (layer_type == \"MaxPool2d\")\n",
    "            is_avgpool = (layer_type == \"AvgPool2d\")\n",
    "            is_leaky = (layer_type == \"LeakyReLU\")\n",
    "            is_elu = (layer_type == \"ELU\")\n",
    "            return [\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_leaky),\n",
    "                gr.update(visible=is_elu),\n",
    "            ]\n",
    "\n",
    "        layer_type_dropdown.change(\n",
    "            toggle_fields,\n",
    "            inputs=[layer_type_dropdown],\n",
    "            outputs=[\n",
    "                conv_kernel, conv_padding, conv_stride, conv_bias,\n",
    "                pool_kernel, pool_stride, pool_padding,\n",
    "                avgpool_kernel, avgpool_stride, avgpool_padding,\n",
    "                leaky_relu_slope, elu_alpha\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        reset_btn = gr.Button(\"Reset Layers\")\n",
    "        reset_btn.click(fn=reset_layers, inputs=[], outputs=builder_arch)\n",
    "\n",
    "        gr.Markdown(\"### Edit or Delete a Layer\")\n",
    "        layer_index = gr.Number(label=\"Layer Index (0-based)\", precision=0)\n",
    "        new_layer_type = gr.Dropdown(list(layer_map.keys()), label=\"New Layer Type\")\n",
    "        new_in_dim = gr.Textbox(label=\"New Input Dim (if applicable)\")\n",
    "        new_out_dim = gr.Textbox(label=\"New Output Dim (if applicable)\")\n",
    "\n",
    "        edit_kernel = gr.Textbox(label=\"Kernel Size\", value=\"3\", visible=False)\n",
    "        edit_padding = gr.Textbox(label=\"Padding\", value=\"1\", visible=False)\n",
    "        edit_stride = gr.Textbox(label=\"Stride\", value=\"1\", visible=False)\n",
    "        edit_bias = gr.Checkbox(label=\"Include Bias\", value=True, visible=False)\n",
    "\n",
    "        edit_pool_kernel = gr.Textbox(label=\"Pool Kernel Size\", value=\"2\", visible=False)\n",
    "        edit_pool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        edit_pool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        edit_avgpool_kernel = gr.Textbox(label=\"AvgPool Kernel Size\", value=\"2\", visible=False)\n",
    "        edit_avgpool_stride = gr.Textbox(label=\"Stride\", value=\"2\", visible=False)\n",
    "        edit_avgpool_padding = gr.Textbox(label=\"Padding\", value=\"0\", visible=False)\n",
    "\n",
    "        edit_leaky_relu_slope = gr.Textbox(label=\"Negative Slope\", value=\"0.01\", visible=False)\n",
    "        edit_elu_alpha = gr.Textbox(label=\"ELU Alpha\", value=\"1.0\", visible=False)\n",
    "\n",
    "        edit_btn = gr.Button(\"Edit Layer\")\n",
    "        delete_btn = gr.Button(\"Delete Layer\")\n",
    "        insert_btn = gr.Button(\"Insert New Layer\")\n",
    "\n",
    "        edit_btn.click(\n",
    "            fn=update_layer,\n",
    "            inputs=[\n",
    "                layer_index, new_layer_type, new_in_dim, new_out_dim,\n",
    "                edit_kernel, edit_padding, edit_stride, edit_bias,\n",
    "                edit_pool_kernel, edit_pool_stride, edit_pool_padding,\n",
    "                edit_avgpool_kernel, edit_avgpool_stride, edit_avgpool_padding,\n",
    "                edit_leaky_relu_slope, edit_elu_alpha\n",
    "            ],\n",
    "            outputs=builder_arch\n",
    "        )\n",
    "\n",
    "        delete_btn.click(fn=delete_layer, inputs=[layer_index], outputs=builder_arch)\n",
    "        insert_btn.click(\n",
    "            fn=insert_layer,\n",
    "            inputs=[\n",
    "                layer_index, new_layer_type, new_in_dim, new_out_dim,\n",
    "                edit_kernel, edit_padding, edit_stride, edit_bias,\n",
    "                edit_pool_kernel, edit_pool_stride, edit_pool_padding,\n",
    "                edit_avgpool_kernel, edit_avgpool_stride, edit_avgpool_padding,\n",
    "                edit_leaky_relu_slope, edit_elu_alpha\n",
    "            ],\n",
    "            outputs=builder_arch\n",
    "        )\n",
    "\n",
    "        def toggle_edit_fields(layer_type):\n",
    "            is_conv = (layer_type == \"Conv2d\")\n",
    "            is_pool = (layer_type == \"MaxPool2d\")\n",
    "            is_avgpool = (layer_type == \"AvgPool2d\")\n",
    "            is_leaky = (layer_type == \"LeakyReLU\")\n",
    "            is_elu = (layer_type == \"ELU\")\n",
    "            return [\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_conv),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_pool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_avgpool),\n",
    "                gr.update(visible=is_leaky),\n",
    "                gr.update(visible=is_elu),\n",
    "            ]\n",
    "\n",
    "        new_layer_type.change(\n",
    "            toggle_edit_fields,\n",
    "            inputs=[new_layer_type],\n",
    "            outputs=[\n",
    "                edit_kernel, edit_padding, edit_stride, edit_bias,\n",
    "                edit_pool_kernel, edit_pool_stride, edit_pool_padding,\n",
    "                edit_avgpool_kernel, edit_avgpool_stride, edit_avgpool_padding,\n",
    "                edit_leaky_relu_slope, edit_elu_alpha\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Device info row\n",
    "        with gr.Row():\n",
    "            device_output = gr.Markdown(get_device_status())\n",
    "            refresh_button = gr.Button(\"üîÑ Refresh Device Status\")\n",
    "            refresh_button.click(fn=get_device_status, inputs=[], outputs=device_output)\n",
    "\n",
    "    with gr.Tab(\"Train\"):\n",
    "        gr.Markdown(\"## Train the Model\")\n",
    "\n",
    "        # Same inputs as before\n",
    "        loss_dropdown = gr.Dropdown(['MSELoss', 'CrossEntropyLoss'], label='Loss Function')\n",
    "        opt_dropdown = gr.Dropdown(['SGD', 'Adam'], label='Optimizer')\n",
    "        lr_box = gr.Textbox(value=\"0.01\", label=\"Learning Rate\")\n",
    "        batch_box = gr.Textbox(value=\"32\", label=\"Batch Size\")\n",
    "        size_box = gr.Textbox(value=\"28\", label=\"Image Resize (e.g. 28x28)\")\n",
    "\n",
    "        # This is still the old \"Path to CSV or ZIP\" input\n",
    "        file_box = gr.Textbox(label=\"Path to CSV or ZIP\")\n",
    "\n",
    "        # The user can optionally override the default path:\n",
    "        custom_box = gr.Textbox(label=\"Custom Extraction Path (optional)\")\n",
    "\n",
    "        epochs_box = gr.Textbox(value=\"100\", label=\"Epochs\")\n",
    "        generate_3d_checkbox = gr.Checkbox(label=\"Generate 3D Descent Animation (‚ö†Ô∏è Slower, CPU/RAM-intensive)\", value=False)\n",
    "        channel_dropdown = gr.Dropdown([1, 3], label=\"Input Channels (1 = Grayscale, 3 = RGB)\", value=3)\n",
    "\n",
    "        # Outputs\n",
    "        loss_curve = gr.Image(label=\"Loss Curve\")\n",
    "        animation_video = gr.Video(label=\"3D Descent Animation\")\n",
    "        model_file = gr.File(label=\"Download Trained Model\")  # <-- user can download from ANY drive they want\n",
    "        log_box = gr.Markdown(label=\"Log\")\n",
    "\n",
    "        # Use our wrapper so we can insert a default path if custom_box is empty:\n",
    "        train_button = gr.Button(\"Start Training\")\n",
    "        train_button.click(\n",
    "            fn=train_model_with_default_path,\n",
    "            inputs=[\n",
    "                loss_dropdown,\n",
    "                opt_dropdown,\n",
    "                lr_box,\n",
    "                batch_box,\n",
    "                size_box,\n",
    "                file_box,\n",
    "                custom_box,\n",
    "                epochs_box,\n",
    "                channel_dropdown,\n",
    "                generate_3d_checkbox\n",
    "            ],\n",
    "            outputs=[\n",
    "                loss_curve,\n",
    "                animation_video,\n",
    "                model_file,\n",
    "                builder_arch,\n",
    "                log_box\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Queue + Launch\n",
    "dashboard.queue()\n",
    "dashboard.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
